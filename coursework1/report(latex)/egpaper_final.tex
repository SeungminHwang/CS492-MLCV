\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{MLCV Coursework 1 Reports}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   The ABSTRACT is to be in fully-justified italicized text, at the top
   of the left-hand column, below the author and affiliation
   information. Use the word ``Abstract'' as the title, in 12-point
   Times, boldface type, centered relative to the column, initially
   capitalized. The abstract is to be in 10-point, single-spaced type.
   Leave two blank lines after the Abstract, then begin the main text.
   Look at previous CVPR abstracts to get a feel for style and length.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Computationally Efficient Eigenfaces} \label{low pca}
Eigen faces are basis set of training image dataset. By using them, we can reconstruct or recognize faces. To make eigen faces of given images, we have to apply PCA to covariance matrix of images. This operation is quite time and space consuming while PCA needs cubic of feature dimension time for its computation. To solve this problem, we employed low-dimensional PCA, which can be used when number of training dataset is quite smaller than dimension of feature.

%-------------------------------------------------------------------------
\subsection{Comparison between PCA and low-dimensional PCA} \label{comp}

\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.8\linewidth]{fig1.png}
\end{center}
   \caption{Plot of nonzero eigenvalues of PCA and low-dimensional PCA
(low-PCA). Nonzero eigenvalues of each matrix are identical.}
\label{fig:eigen}
\end{figure}

As result, number of nonzero eigenvalues in covariance matrix 
$S=\frac{1}{N} A A^{T}$ is same with number of nonzero eigenvalues in
 matrix $\frac{1}{N} A ^{T}A$. Moreover, as we can see in 
Figure~\ref{fig:eigen} nonzero eigenvalues of each matrix are same. Also,
 if we multipy A to eigenvectors of low-dimensional matrix, they become same
 with eigenvectors of covariance matrix. However, while their resulting values
 and vectors are identical, their time complexities are different. Let feature
 dimension D and number of training data N. Then, computing covariance
 matrix and finding eigenvalues and eigenvectors of covariance matrix S
 takes $O(N D^{2} + D^{3})$. Low-dimensional PCA takes 
$O(2N^{2}D + N^{3})$ to compute low-dimensional matrix, find eigenvectors,
 eigenvalues and multiply A to eigenvectors. So, if D is much bigger than N,
 low-dimensional PCA takes less computation time. However, if N is
 compatible with D, additional matrix multiplying with A might make 
low-dimensional PCA takes more time than original PCA. In our case, while
 N(416) is relatively smaller than D(2576), low-dimensional PCA takes less
 time to compute eigenfaces. Table~\ref{table:results} shows and compare
 training time of PCA and low-dimensional PCA.



\begin{table*}
\begin{center}
\begin{tabular}{|l|ccc|}
\hline
Method & Training Time(sec) & Reconstruction Error & Recognition Accuracy(\%) \\
\hline\hline
PCA & 9.36 & - & - \\
low-PCA & 0.79 & 11.93 & 62.5 \\
PCA-LDA & 17.50 & X & 92.3 \\
subset PCA & 0.14 & 22.84 & 61.5 \\
Incremental PCA & 0.54 & 18.46 & 62.5\\
\hline
\end{tabular}
\end{center}
\caption{Results. Reconstruction error is measured by rmse with $m_{pca} = 50$.
 Recognition Accuracy is measured with $m_{pca} = 50$ for low-PCA, subset PCA and incremental PCA. $m_{pca} = 100, m_{lda} = 50$ is used for PCA-LDA}
\label{table:results}
\end{table*}

\subsection{Face Reconstruction}
With eigenfaces optained from above section, we can reconstruct any given faces. Specifically,
 we optained basis of training face data and any face can be projected to our face space.
Then, we can express those faces by linear combination of several eigenfaces. There are 415
eigenvectors with nonzero eigenvalues. Indeed, we can reconstruct all faces in training data exactly
 with 415 eigenfaces. Figure~\ref{fig:sfig1}~\ref{fig:sfig2}~\ref{fig:sfig3} shows using more eigenfaces
can make more smillar faces and using 415 eigenfaces could reconstruct almost same face.
 However, several faces in test data cannot be reconstructed exactly, because 
test face could not be included in train face space. By comparing Figure~\ref{fig:sfig3} and Figure~\ref{fig:sfig4},
we can see that Figure~\ref{fig:sfig3} is reconstructed correctly with 415 bases, while
Figure~\ref{fig:sfig4} failed with same number of bases.

\begin{figure*}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{fig4.png}
  \caption{Reconstructed face with 10 bases}
  \label{fig:sfig1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{fig5.png}
  \caption{Reconstructed face with 100 bases}
  \label{fig:sfig2}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{fig6.png}
  \caption{Reconstructed face with 415 bases}
  \label{fig:sfig3}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{fig3.png}
  \caption{Failed image}
  \label{fig:sfig4}
\end{subfigure}
\caption{Plots of reconstructed faces.}
\label{fig:recon face}
\end{figure*}
%--------------------------------------------------------------------------
\section{PCA-LDA for Face Recognition}

LDA increase variance of features between other classes and decrease variance 
between same classes. In this section, we explore the effect of apllying LDA over
 PCA on face recognition accuracy in many aspects. Classifier used in this section
 is KNeighborsClassifier of python scipy library, which employed default settings.

\subsection{Recognition accuracy}
Let's first talk about recognition accuracy with PCA only method. As we can see in Figure~\ref{fig:acc PCA}, accuracy of 62\% is achieved with 50 bases and saturated. On the other hand, recognition accuracy of PCA-LDA method is quite interesting. Using higher $m_{pca}$ increases accuracy of recognition, because it can represent each face more accurately. Peak accuracy is achieved with $m_{pca} = 364$ and $m_{lda} = 31$(100\%), and it suddenly fall down with higher $m_{lda}$. It could be caused by curse of dimension. Rank of within-class scatter matrix is $N - n_{class} = 416-52 = 364$ and rank of between-class scatter matrix is $n_{class} - 1 = 52 - 1 = 51$. While LDA has atmost rank of $(W^T_{PCA}S_W W_{PCA})^{-1}(W^T_{PCA}S_B W_{PCA})$nonzero eigenvectors, it has 51 nonzero eigenvectors. It means using $m_{lda}$ larger than 51 cannnot increase variance of between-class features and decrease variance  of within-class features. While variance is fixed, increasing dimension complicates classificatin because nn classification is based on eucledian distance, but too high dimension makes distance too large, eventhough they are in same class.

\begin{figure}
\begin{center}
   \includegraphics[width=0.8\linewidth]{fig9.png}
\end{center}
   \caption{Accuracy of face recognition with PCA}
\label{fig:acc PCA}
\end{figure}


\begin{figure}
\begin{center}
   \includegraphics[width=0.8\linewidth]{fig7.png}
\end{center}
   \caption{Accuracy of face recognition with PCA-LDA}
\label{fig:acc PCA-LDA}
\end{figure}

\subsection{Example of success and fail faces}

There are few examples of success and fail cases. Figure~\ref{fig:success} shows successfully predicted faces. While test face in first row of Figure~\ref{fig:success} seems sharper than other faces in same class, our classifier successfully predict its class. In failure cases(Figure~\ref{fig:fail}), test face have simillar hairstyle with faces in wrong predicted class. It could be hard to classify them with other high accuracy classifiers.

\begin{figure}
\begin{center}
   \includegraphics[width=0.8\linewidth]{fig11.png}
\end{center}
   \caption{Predict successed faces. First column shows test faces and second to fourth columns shows train faces included in class of successfully predicted.}
\label{fig:success}
\end{figure}

\begin{figure}
\begin{center}
   \includegraphics[width=0.8\linewidth]{fig10.png}
\end{center}
   \caption{Predict failed faces. First column shows test faces and second to fourth columns shows train faces included in class of wrong predicted}
\label{fig:fail}
\end{figure}

\subsection{Time and memory}

Table~\ref{table:results} briefly compare computation time of PCA and PCA-LDA methods with fixed parameters. As explained in Section~\ref{comp}, low-PCA takes $O(2N^{2}D + N^{3})$ time. However, LDA takes $O(NDmin(N,D) + \min(N,D)^3)$ time complexity and $O(ND + N\min(N,D)+D\min(N,D))$ memory. PCA-LDA has strong tradeoff between accuracy and time-memory comsumption.
%-------------------------------------------------------------------------
\section{Incremental PCA}

In the reality, there could be case that we could not get full training data at once and given with some time gap. With normal PCA or low-dimensional PCA, we have to train model from the bottom if training data is transformed. Incremental PCA is useful strategy for such data transformation. We also can reduce training time with Incremental PCA.

\subsection{Comparison with other PCAs}

There are normal PCA, low-dimensional PCA, PCA-LDA, PCA with one subset and incremental PCA. In our case, we want to deal with situation of gradual data incoming. So there are three PCA we can use in this situation: low-PCA, PCA with first subset and incremental PCA. In Table~\ref{table:results}, we now can compare these PCA strategies. Using one subset PCA is fast, but it have big reconstruction error and low recognition accuracy. This is because small data brings low rank face space and low accuracy reconstruction of faces in test dataset. Low-dimensional PCA has highest reconstruction error and lowest recognition accuracy. Also, it seems time consumption is very compatitable with subset PCA and incremental PCA. However, its fact only with much low number of dataset than face dimension. As we can see in comparison for PCA and one subset PCA, training time cubicly increases. But incremental PCA does not. It have reasonable reconstruction error and recognition accuracy. Also, it's training time does not increase cubicly, and it is sum of all 4 subset PCA and time for merge them: it means, in real situation, we need very few time to merge new data with existing PCA model.   




{\small
\bibliographystyle{ieee}
%\bibliography{egbib}
}

\end{document}
