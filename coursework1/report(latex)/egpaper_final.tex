\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{kotex}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{MLCV Coursework 1 Report}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Seungmin Hwang\\
20180736\\
Department of Physics\\
{\tt\small min.hwang@kaist.ac.kr}
}

\maketitle
%\thispagestyle{empty}


%%%%%%%%% BODY TEXT
\section{Computationally Efficient Eigenfaces}
\section{PCA-LDA for Face Recognition}
\section{Incremental PCA}


\section{K-means Codebook}
\subsection{Visual codebook generation wiht K-means clustering}
By applying dense SIFT feature description, features descriptor vectors can be obtained from the given dataset (Caltech-101). There are lots of descriptors from this process. But Itâ€™s more convenient to consider only randomly selected 100K descriptors. It will save memory and the computation time.\\

In this dataset, whole number of descriptors form 150 images is around 410K, which order is almost same with 100K. Therefore, the information loss from randomly selecting 100K descriptor is not too much.\\

Then, with certain number $K$ and 100K descriptor vector, by using K-means clustering, the $K$ representative codeword for this dataset can be obtained. Let d be the dimension of the descriptor vector. Firstly, pick $K$ random $d$ dimensional vector for center of each clusters. Then, assign the descriptor vectors to the nearest center and update the cluster center by calculating the mean value of the cluster. Repeat this assign-update process until the cluster center converges.\\

This will be visual codebook generated with K-means clustering.\\
This visual codebook has $K$ codewords and each word is $d$ dimensional vector.\\


\subsection{Vector quantisation process}
In the previous section, descriptor vectors can be obtained from images. Let's apply this process to the single image. Then we can get the set of $N$ descriptor vectors where $N$ can be different by the images.\\
With this set of descriptors and centroids from K-means clustering, we can apply k-Nearest Neighbor (kNN) algorithms to get certain vector.\\








\subsection{Computation time analysis}
The time complexity of K-means clustering is known as $O(N'KD)$ where $N'$ is number of the descriptor vectors (data point), $K$ is number of clusters and $D$ is the dimension of the descriptor vector.\\

And the vector quantization process for the train data and query data,


\begin{figure}[h]
    \centering
    \includegraphics[width=0.30\textwidth]{./assets/k-means_graph.png}
    \caption{Computation time for each process}
    \label{fig:k-means-time}
\end{figure}




\subsection{Bag-of-words histogram of images}
From the previous process, $K$ dimensional vector can be obtained from each image. The components of this vector are the frequency of correponding codeword. That is, if there are two different images but in the same class, they are more likely to have common codewords. // TODO:\\

Figure \ref{fig:k-means-hist-1} is sample images and corresponding bag-of-words histograms with $K = 64$.


Let's do some qulitative analysis.\\
From the figure \ref{fig:k-means-hist-1} (a), among the trilobite images, we can find that there exist significant codewords that specify the image is trilobite.\\
However, in the Figure \ref{fig:k-means-hist-1} (b), it seems there exist significant codeword if we only consider right two images, but if we include the left most image, it's hard to say unique feature.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.25\textwidth]{./assets/kmeans_hist_1.png}\\
    (a)\\
    \includegraphics[width=0.25\textwidth]{./assets/kmeans_hist_2.png}\\
    (b)
    \caption{Sample image and corresponding histogram}
    \label{fig:k-means-hist-1}
\end{figure}

Then, what about value of $K$ lower and higher than 64?

\begin{figure}[h]
    \centering
    \includegraphics[width=0.25\textwidth]{./assets/K4_kmeans.png}
    \caption{Computation time for each process}
    \label{fig:K4_kmeans}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.25\textwidth]{./assets/K512_kmeans.png}
    \caption{Computation time for each process}
    \label{fig:K512_kmeans}
\end{figure}



\section{RF Classifier}
\subsection{RF classifier with K-means codebook}
Random Forest classifier (RF classifier) is the randomised decision forest which performs classification. The RF for classification can be build with set of data vectors with label. Randomisation processes are done in two ways: randomising training set and randomised feature selection. Mostly, these increase the generallity.

//TODO

The train and test data can be obtained from the result of Section 4. In this section, we obtained bag of visual words histogram for each images from the Caltech-101 dataset.\\
Let $d$ be the cardinality of K-means codebook, the the histogram data can be $d$ dimensional vector, which components are frequencies of correspoding codeword. And there's a label which specifies what this image is.\\
With this data and label, we can apply RF classifer to solve classification problems.





\subsection{Vocabularay size and RF classifier}
The RF classification performances(memory, execution time, accuraccy and so forth) can be also affected by the size of visual vocabulary.

Let's consider the random forest which parameter has default value of:
\begin{table}[h]
    \begin{center}
        \begin{tabular}{|l|c|}
        \hline
        Parameter & Default Value\\
        \hline\hline
        Number of the Trees & 100 \\
        Number of depths & 5 \\
        Split Number & 3\\
        Objective Function & IG\\
        Weak Learner & Axis aligned\\
        \hline
        \end{tabular}
    \end{center}
    \caption{Default parameters of Random Forest}
    \label{table:defaultParams}
\end{table}

Figure \ref{fig:numBinsRFperform} shows that the test accuracy is almost monotonically increasing about vocabulary size. At $K = 8$, the mean test accuracy was 0.49068. When the vocabulary size is doubled($K = 16$) the mean test accuracy is increased by 0.6220. But, since the increasing rate of accuracy is rapidly goes to zero, for the sufficiently large vocabulary size, the size will not affect to the accuracy while the training time is higly increased.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.22\textwidth]{./assets/Q5/numBins_RF_time.png}
    \includegraphics[width=0.22\textwidth]{./assets/Q5/numBins_RF_accu.png}
    \caption{ Consumed time (left), accuracy (right)}
    \label{fig:numBinsRFperform}
\end{figure}

Figure \ref{fig:numBinsConfusion} shows that how many tests was successful by each classes of images. Classifier with 8 visual words has 6 classes with accuracy less than 0.5. By contrast, Classifier with 1024 visual words has only 2 classes with accuracy less thant 0.5. This demonstrates that better classifier can be obtained by using larger codebook.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.22\textwidth]{./assets/Q5/numBinsConfusionK8.png}
    \includegraphics[width=0.22\textwidth]{./assets/Q5/numBinsConfusionK1024.png}
    \caption{Confusion matrix of $K=8$ (left), $K=1024$ (right)}
    \label{fig:numBinsConfusion}
\end{figure}



\subsection{Parameters for RF classifier}

The Randomized Descision Forest(RF) can be adjusted by the value of parameters. In our RF model, there are parameters: number of trees, number of depth, type of weak learners and so forth.\\
The result on the below is how the computation time and accuracy will be changed if the value of some parameter is changed. Let's consider the forest with parameters are same in the Table \ref{table:defaultParams}.

\subsubsection{Number of Trees}


The single decision tree tends to highly overfit to data. To prevent, using ensembles of slightly different tree will be very helpful for generalisation.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.22\textwidth]{./assets/Q5/numTree_time.png}
    \includegraphics[width=0.22\textwidth]{./assets/Q5/numTree_accuracy.png}
    \caption{Sample image and corresponding histogram}
    \label{fig:analysis-numTree}
\end{figure}


In the Figure \ref{fig:analysis-numTree}, the number of trees is changed from 1 to 1000. For small number like 1 has very low test accuracy. But if we slightly increse the number of tree by 100, then the test accuracy is increased to around 0.6. After 100, the test accuracy is just fluctuating around 0.6.

The computation time for training is increasing linearly in number of trees and for testing is increasing logarithmically. This results agree with expectation since the 

The best choice of this parameter is around 100.

\subsubsection{Depth of Trees}

Depth of Trees parameter is the maximum depth of trees. This paramteter can be one of stopping criteria when the RF is fistly generated with training data. This paramter decides the tree structures. 
Depth of Trees is the paramter deciding the tree structures. This paramter provides stopping criteria when the RF is fistly generated.\\
In the Figure \ref{fig:analysis-numDepth}, the test accuracy is gradually increasing over the depth but the training time grows exponetial order. 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.22\textwidth]{./assets/Q5/numDepth_time.png}
    \includegraphics[width=0.22\textwidth]{./assets/Q5/numDepth_accu.png}
    \caption{Sample image and corresponding histogram}
    \label{fig:analysis-numDepth}
\end{figure}




\subsubsection{Split Number}
When the RF classifier be built, node split task is done for non-terminal nodes. In the node plit task, we just randomly select one component of vector, compare with the thresshold. This task done by $\rho$ time, which called the randomness parameter.\\
Since the one component of data vector does not have entire data, $\rho$ should be certain number of larger than 1. Figure \ref{fig:analysis-SplitNum} shows the result with many different $\rho$ vaules.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.22\textwidth]{./assets/Q5/splitNum_time.png}
    \includegraphics[width=0.22\textwidth]{./assets/Q5/splitNum_accu.png}
    \caption{Sample image and corresponding histogram}
    \label{fig:analysis-SplitNum}
\end{figure}
For $\rho = 3, 10, 50$, the accuracy fluctuating around 0.6, and after that the accuracy is gradually falls while the training time grows linearly.
Once $\rho$ increases, randomness decreases. If $\rho$ is sufficiently large number, then all of the tree has identical shapes.

\subsubsection{Type of Weak Learner}
Until now, all of the calculation was based on axis-aligned split function. Let's compare the performance of each split function. RF classification result is 
\begin{table}[ht]
    \centering
    \begin{center}
        two pixel\\
        \begin{tabular}{|l|c|c|}
        \hline
        Index & Time (s) & stdev \\
        \hline\hline
        Train time & 2.19918 & 0.113488202\\
        Test time & 0.18562 & 0.016719211\\
        Accuracy & 0.55602 & 0.020343721\\
        \hline
        \end{tabular}
    \end{center}
    \begin{center}
        axis-aligned\\
        \begin{tabular}{|l|c|c|}
        \hline
        Index & Time (s) & stdev \\
        \hline\hline
        Train time & 2.40044 & 0.070384324\\
        Test time & 0.12416 & 0.003166702\\
        Accuracy & 0.6720 & 0.009893938\\
        \hline
        \end{tabular}
    \end{center}

    \begin{center}
        Linear\\
        \begin{tabular}{|l|c|c|}
        \hline
        Index & Time (s) & stdev \\
        \hline\hline
        Train time & 3.2615 & 0.172281041\\
        Test time & 0.1952 & 0.14564903\\
        Accuracy & 0.5933 & 0.009900657\\
        \hline
        \end{tabular}
    \end{center}
    \caption{Performance of two split functions}
    
    \label{table:cat_su}
\end{table}



\subsection{Success and Failure Analysis}
Figure \ref{fig:numBinsConfusion} demontsrates this RF classifier is especially weak for the class umbrella(3) and wild\_cat(7). Let's investigate success and failure cases. For more cases, please refer to the appendix.
\begin{table}[ht]
    \centering
    \begin{center}
        \begin{tabular}{|l|c|c|}
        \hline
        Test Image & True Class & predicted class\\
        \hline\hline
        \includegraphics[width=0.1\textwidth]{./assets/Q5/su/cat_s1.png} & 7 & 7 \\
        \hline
        \includegraphics[width=0.1\textwidth]{./assets/Q5/su/cat_s2.png} & 7 & 7\\
        \hline
        \includegraphics[width=0.1\textwidth]{./assets/Q5/su/cat_s3.png} & 7 & 7\\
        \hline
        \includegraphics[width=0.1\textwidth]{./assets/Q5/su/cat_f1.png} & 7 & 6\\
        \hline
        \includegraphics[width=0.1\textwidth]{./assets/Q5/su/cat_f2.png} & 7 & 5\\
        \hline
        \end{tabular}
    \end{center}
    \caption{Sucess and failure case examples}
    
    \label{table:cat_su}
\end{table}

\begin{table}[ht]
    \centering
    \begin{center}

        \begin{tabular}{|l|c|c|}
        \hline
        Test Image & True Class & predicted class\\
        \hline\hline
        \includegraphics[width=0.1\textwidth]{./assets/Q5/su/umb_s1.png} & 3 & 3 \\
        \hline
        \includegraphics[width=0.1\textwidth]{./assets/Q5/su/umb_s2.png} & 3 & 3\\
        \hline
        \includegraphics[width=0.1\textwidth]{./assets/Q5/su/umb_s3.png} & 3 & 3\\
        \hline
        \includegraphics[width=0.1\textwidth]{./assets/Q5/su/umb_f1.png} & 3 & 5\\
        \hline
        \includegraphics[width=0.1\textwidth]{./assets/Q5/su/umb_f2.png} & 3 & 10\\
        \hline
        \end{tabular}
    \end{center}
    \caption{Sucess and failure case examples}
    
    \label{table:umb_su}
\end{table}

From Figure \ref{table:cat_su}, it seems like this RF classifier is better at the image of wild\_cat containing whole cat bodies.

From Figure \ref{table:umb_su}, this RF classifier is better at umbrella which has triangular stripes. Otherwise, it categorise umbrella image into wrong class.








{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
